---
base_model: NousResearch/Llama-2-13b-hf
tags:
- generated_from_trainer
model-index:
- name: qlora-out-3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

[<img src="https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png" alt="Built with Axolotl" width="200" height="32"/>](https://github.com/OpenAccess-AI-Collective/axolotl)
# qlora-out-3

This model is a fine-tuned version of [NousResearch/Llama-2-13b-hf](https://huggingface.co/NousResearch/Llama-2-13b-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.0789

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2
- eval_batch_size: 2
- seed: 42
- distributed_type: multi-GPU
- num_devices: 2
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- total_eval_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 10
- num_epochs: 3

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.3608        | 0.03  | 20   | 0.3462          |
| 0.1859        | 0.06  | 40   | 0.1161          |
| 0.1526        | 0.09  | 60   | 0.1074          |
| 0.1686        | 0.12  | 80   | 0.1080          |
| 0.14          | 0.15  | 100  | 0.1054          |
| 0.1875        | 0.18  | 120  | 0.1339          |
| 0.114         | 0.2   | 140  | 0.0976          |
| 0.1237        | 0.23  | 160  | 0.0953          |
| 0.11          | 0.26  | 180  | 0.0833          |
| 0.1701        | 0.29  | 200  | 0.0921          |
| 0.1742        | 0.32  | 220  | 0.0882          |
| 0.1303        | 0.35  | 240  | 0.0816          |
| 0.131         | 0.38  | 260  | 0.0883          |
| 0.1421        | 0.41  | 280  | 0.0852          |
| 0.1362        | 0.44  | 300  | 0.0805          |
| 0.1993        | 0.47  | 320  | 0.0856          |
| 0.1428        | 0.5   | 340  | 0.0807          |
| 0.1907        | 0.53  | 360  | 0.0826          |
| 0.2226        | 0.55  | 380  | 0.0784          |
| 0.1377        | 0.58  | 400  | 0.0825          |
| 0.1231        | 0.61  | 420  | 0.0800          |
| 0.1288        | 0.64  | 440  | 0.0852          |
| 0.1495        | 0.67  | 460  | 0.0766          |
| 0.1727        | 0.7   | 480  | 0.0828          |
| 0.1036        | 0.73  | 500  | 0.0727          |
| 0.1763        | 0.76  | 520  | 0.0782          |
| 0.0799        | 0.79  | 540  | 0.0948          |
| 0.1534        | 0.82  | 560  | 0.0757          |
| 0.1287        | 0.85  | 580  | 0.0778          |
| 0.1098        | 0.88  | 600  | 0.0795          |
| 0.1388        | 0.91  | 620  | 0.0811          |
| 0.1357        | 0.93  | 640  | 0.0764          |
| 0.1646        | 0.96  | 660  | 0.0737          |
| 0.1402        | 0.99  | 680  | 0.0742          |
| 0.0975        | 1.02  | 700  | 0.0720          |
| 0.1276        | 1.05  | 720  | 0.0785          |
| 0.122         | 1.08  | 740  | 0.0774          |
| 0.1082        | 1.11  | 760  | 0.0705          |
| 0.1261        | 1.14  | 780  | 0.0720          |
| 0.1169        | 1.17  | 800  | 0.0695          |
| 0.1186        | 1.2   | 820  | 0.0738          |
| 0.1583        | 1.23  | 840  | 0.0759          |
| 0.0959        | 1.26  | 860  | 0.0739          |
| 0.1121        | 1.28  | 880  | 0.0783          |
| 0.1116        | 1.31  | 900  | 0.0744          |
| 0.1202        | 1.34  | 920  | 0.0717          |
| 0.1142        | 1.37  | 940  | 0.0777          |
| 0.1181        | 1.4   | 960  | 0.0805          |
| 0.1057        | 1.43  | 980  | 0.0803          |
| 0.1601        | 1.46  | 1000 | 0.0760          |
| 0.1369        | 1.49  | 1020 | 0.0763          |
| 0.0748        | 1.52  | 1040 | 0.0812          |
| 0.1149        | 1.55  | 1060 | 0.0803          |
| 0.0871        | 1.58  | 1080 | 0.0803          |
| 0.0933        | 1.61  | 1100 | 0.0766          |
| 0.0858        | 1.64  | 1120 | 0.0721          |
| 0.0964        | 1.66  | 1140 | 0.0769          |
| 0.1911        | 1.69  | 1160 | 0.0713          |
| 0.089         | 1.72  | 1180 | 0.0739          |
| 0.0628        | 1.75  | 1200 | 0.0751          |
| 0.0825        | 1.78  | 1220 | 0.0776          |
| 0.1354        | 1.81  | 1240 | 0.0740          |
| 0.1186        | 1.84  | 1260 | 0.0732          |
| 0.1331        | 1.87  | 1280 | 0.0739          |
| 0.1142        | 1.9   | 1300 | 0.0777          |
| 0.1571        | 1.93  | 1320 | 0.0739          |
| 0.1316        | 1.96  | 1340 | 0.0743          |
| 0.0781        | 1.99  | 1360 | 0.0782          |
| 0.0626        | 2.01  | 1380 | 0.0744          |
| 0.0517        | 2.04  | 1400 | 0.0752          |
| 0.0831        | 2.07  | 1420 | 0.0795          |
| 0.0669        | 2.1   | 1440 | 0.0835          |
| 0.0982        | 2.13  | 1460 | 0.0762          |
| 0.0716        | 2.16  | 1480 | 0.0746          |
| 0.0805        | 2.19  | 1500 | 0.0760          |
| 0.0751        | 2.22  | 1520 | 0.0805          |
| 0.0432        | 2.25  | 1540 | 0.0796          |
| 0.0705        | 2.28  | 1560 | 0.0772          |
| 0.088         | 2.31  | 1580 | 0.0782          |
| 0.1061        | 2.34  | 1600 | 0.0779          |
| 0.0697        | 2.36  | 1620 | 0.0790          |
| 0.0716        | 2.39  | 1640 | 0.0826          |
| 0.049         | 2.42  | 1660 | 0.0813          |
| 0.0808        | 2.45  | 1680 | 0.0793          |
| 0.0989        | 2.48  | 1700 | 0.0775          |
| 0.0996        | 2.51  | 1720 | 0.0764          |
| 0.09          | 2.54  | 1740 | 0.0765          |
| 0.1121        | 2.57  | 1760 | 0.0772          |
| 0.1282        | 2.6   | 1780 | 0.0783          |
| 0.0592        | 2.63  | 1800 | 0.0782          |
| 0.0848        | 2.66  | 1820 | 0.0782          |
| 0.0691        | 2.69  | 1840 | 0.0787          |
| 0.064         | 2.72  | 1860 | 0.0788          |
| 0.0885        | 2.74  | 1880 | 0.0787          |
| 0.055         | 2.77  | 1900 | 0.0785          |
| 0.0658        | 2.8   | 1920 | 0.0789          |
| 0.0887        | 2.83  | 1940 | 0.0789          |
| 0.0537        | 2.86  | 1960 | 0.0786          |
| 0.0237        | 2.89  | 1980 | 0.0790          |
| 0.0798        | 2.92  | 2000 | 0.0790          |
| 0.106         | 2.95  | 2020 | 0.0787          |
| 0.0854        | 2.98  | 2040 | 0.0789          |


### Framework versions

- Transformers 4.35.0.dev0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.0
